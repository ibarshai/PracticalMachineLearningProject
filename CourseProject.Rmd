## Practical Machine Learning Course Project
## Predicting the Quality of Activity
### Summary
For this course project, we were working with a large dataset created by Groupware@LES. Six participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:

- A - exactly correctly
- B - throwing the elbows to the front
- C - lifting the dumbbell only halfway
- D - lowering the dumbbell only halfway 
- E - throwing the hips to the front

Their activity was measured with sensors at the belt, upper arm and forearm.

Using this data, we were to develop an algorithm do identify the classification of activity quality based on the sensor outputs.

In the end, I found that using a Random Forest algorithm was incredibly accurate and, with some very minor tuning, highly efficient.

### Exploring and Cleaning the Data
I first opened the .csv file and looked at the data. There were 19622 samples of 160 variables provided in the training set. On first glance, it was immediately obvious that many of these variables were missing for most of the samples, and so I started reducing the dimensions of this problem.

Firstly, I removed the name and time variables. Making predictions based on these might be an effective way to predict within this dataset, but would be entirely useless if a test dataset contained other subjects or had the excersized done in a different order. I also removed all the other variables that didn't correspond to sensor outputs.

Second, I removed all the variables that contained very little data. Variables like kurtosis and skewness, as well as the maximums and minimums were removed because they could be inferred from the 3D accelerometer and gyroscope data.

This left me with 53 entirely functional variables to use as predictors, which was much more manageable.

```{r echo=FALSE, tidy=FALSE, results='hide'}
library(caret)
library(randomForest)

# Load the CVS files from active directory
setwd("C:/Users/18662/Desktop/Data Science/8 - Machine Learning/Assignments")
trainingdata = read.csv("pml-training.csv")
testingdata  = read.csv("pml-testing.csv")

# Clean training set to remove all patchy data and convert factor variables to factor type
training = trainingdata[,-c(1:7)]
toBeRemoved = which(training[1,]=="" | is.na(training[1,]))
training = training[,-toBeRemoved]
training$classe = as.factor(training$classe)

# Clean testing set to make it look like training set
testing = testingdata[,-c(1:7)]
testing = testing[,-toBeRemoved]
testing = testing[,1:52]

# Clean up memory
rm(trainingdata); rm(testingdata); rm(toBeRemoved)
```

### Creating the Model
To create the prediction, I chose to use the Random Forest modeling method. This algorithm is typically very accurate and seemed appropriateb because this is a discrete classification problem, which is one where this algorithm excels.

To construct my model, I used the randomForest() function, which I found much simpler to use than the caret train(method="rf") function. I set up a timer to see how long the function would run, set the seed and ran the function on all of my variables.

```{r echo=TRUE}
ptm <- proc.time()

set.seed(321)
modFitRF = randomForest(classe ~ ., data=training)

modelingtime = proc.time() - ptm
```

The untuned Random Forest model used 500 trees, took about 66 seconds to run and created a model with the following specifications:

```{r echo=FALSE}
modFitRF
```

The plot of the error vs number of trees for this model was as follows:
```{r echo=FALSE}
plot(modFitRF)
```

From this plot, we can see that the error stabilizes well before the 500th tree, so we can significatly reduce the number of trees used. In fact, it looks like there is very little benefit in using over 50 trees. Limiting the number of trees to 50 and remodeling yields the following:

```{r echo=TRUE}
set.seed(321)
modFitRF = randomForest(classe ~ ., data=training, ntree=50)
modFitRF
```

This model yields a slightly higher OOB error estimate (0.44% vs 0.29%), but it's still extremely accurate and takes about 7 seconds to run, which is a massive performance boost.

The trees vs error curve shows that we could probably reduce the number of trees a little more without much of an accuracy decrease, but with the current runtime, there is little reason to compromise for the purpose of this project.

```{r echo=FALSE}
plot(modFitRF)
```

The predictions for the test data, which are all accurate, are as follows:
```{r echo=TRUE}
predictionRF = predict(modFitRF, testing)
predictionRF
```

### Cross Validation

The Random Forest function includes a cross validation algorithm during the tree selection process, so it provide an OOB (out of box) error estimate for out of sample error. For the purpose of this project, I conducted a cross validation to learn.

I chose random sampling to select my cross validation test set. I did so because I did not want to capture any trends that selecting consecutive samples might lead to, based on the fact that many adjascent samples will have been from the same subject and may have a temporal relationship. I set the test set to 10% of the data and looped through the prediction 20 times.

```{r echo=FALSE}
errors = vector()
ratioTestRows = 0.1 #Select ratio of rows to treat as cross-validation test set

for(i in 1:20){
  sampledRows = sample(nrow(training), round(nrow(training)*ratioTestRows), replace=FALSE)
  
  cv.testing = training[sampledRows,]
  cv.training = training[-sampledRows,]

  modFitRFCV = randomForest(classe ~ ., data=cv.training, ntree=50)
  
  predictionCV =predict(modFitRFCV, cv.testing)
  cm = confusionMatrix(cv.testing$classe, predictionCV)
  
  errors[i] = 1-cm$overall['Accuracy']
  print(paste("Error for pass", i, ":", errors[i]))
  
  rm(cv.training); rm(cv.testing); rm(modFitRFCV);
}

print(paste("Average error", ":", mean(errors)))
```
As we can see, the error estimated by the randomForest() function was not optimistic, and actually quite close to the random cross vaidation that I performed manually.

### Other Thoughts
One possible performance improvement could be gained from further reducing the dimensionality of the problem. I believe that this dataset lends itself well to this.

When we construct a correlation marix of all of the variables, we can see that quite a few of them have a correlation of over 0.9.

```{r echo=FALSE}
M = abs(cor(training[,-53]))
diag(M) = 0
Mcor = which(M>0.9, arr.ind=T)
Mcor = Mcor[order(Mcor[,1]),]
McorNames = cbind(names(M[1,][Mcor[,1]]), names(M[2,][Mcor[,2]]))
McorNames
```

From this, we can see that roll_belt is heavily correlated to 3 other variables, therefore, it wouldn't be unreasonable to expect good results from a PCA of these 4 variables, reducing the problem by 3 dimensions while losing very little accuracy. This approach was not necessary for this problem, however, because it's very manageable with a basic cleaning of the dataset and tuning of the modeling algorithm.

THE END